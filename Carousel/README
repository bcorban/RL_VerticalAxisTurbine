Organisation :
2 main files : sac_continuous_action for the cleanrl implementation of the SAC algorithm for training
and RL_/envs/CustomEnv where all the interaction with the turbine is done (loads reading, pitch command, motor E start and stop...)

test_policy allows to evaluate a trained policy
process_main processes the data (enter folder,ms,mpt and if it is a training or evaluation run)

Protocole:
Always start the day by a quick_home, then a non actuated case :
for that set ACTUATE to False in config_ENV.py and the number of total timestep in sac_continuous_action to 5000 and learning_starts to 1000 for example.
then sharx_on(RPM) in matlab, and then launch sac_continuous_action.py. then postprocess this non actuated run with process main (has to be ms001 so that it exports a Cp_phavg.mat used in some rewards for training)

For a training run, set the right reward in the get_transition method, the right parameters in sac_continuous_action

Process
-------

UNACTUATED CASE:
----------------

- Setup parameters in config_ENV.py
  Change the parameters following the requirements for the experiment.

- Setup sac_continuous_action.py
  - Parser:
    --total-timesteps: 50 iterations corresponds roughly to 1 rotation. Usually set to 5000 (100 rotations) for unactuated.
    --learning-start: How many iterations completed before learning starts. Usually set to 5000. Not important for unactuated.

- Run sac_continuous_action.py. Ignore the warnings.

- Do a post-processing step to create a required file.
  - In process_main.m: change:
    - folder
    - ms
    - mpt
    This creates the reference file that will be used for the reward.


ACTUATED CASE:
--------------

- In convig_ENV.py, change:
  - ms
  - Set ACTUATE to True

- In sac_continuous_action.py
  --total-timesteps: 200000 (~1.5hr)
  --buffer-size: set to total-timesteps
  --learning-start: 5000

- In CustomEnv.py
  - Change the reward in the method get_transition()

- Run sac_continuous_action.py. Ignore the warnings.

- run process_main.m from matlab, with the correct ms and mpt. Change is_training to true.

REPLAY POLICY:
--------------

- Find latest policy:
  - In C:\Users\PIVUSER\Desktop\RL_VerticalAxisTurbine\Carousel\wandb\run-LAST RUN\files select an actor (.pt file)

- In test_policy.py:
  - In __main__: Change path and actor name

- In config_ENV.py:
  - Change the ms to a greater value to avoid overwritting.

- run test_policy.py

- run process_main.m from matlab, with the correct ms and mpt. Change is_training to false.



Baptiste Corban and Daniel Fernex
